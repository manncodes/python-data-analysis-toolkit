{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Data Analysis Toolkit - Basic Usage Example\n",
    "\n",
    "This notebook demonstrates basic usage of the datoolkit package for data analysis, visualization, and machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Import functions from datoolkit\n",
    "from datoolkit.stats import descriptive_stats, correlation_analysis\n",
    "from datoolkit.visualization import plot_histogram, plot_correlation_matrix, plot_scatter\n",
    "from datoolkit.preprocessing import normalization, handle_missing_values\n",
    "from datoolkit.ml import train_test_split_stratified, cross_validation_metrics, feature_importance_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Sample Data\n",
    "\n",
    "For this example, we'll use the diabetes dataset from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "feature_names = diabetes.feature_names\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "X = pd.DataFrame(diabetes.data, columns=feature_names)\n",
    "y = pd.Series(diabetes.target, name='target')\n",
    "\n",
    "# Display the first few rows\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Statistical Analysis\n",
    "\n",
    "First, let's calculate descriptive statistics for the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate descriptive statistics for the target variable\n",
    "target_stats = descriptive_stats(y)\n",
    "print(\"Target Variable Statistics:\")\n",
    "for key, value in target_stats.items():\n",
    "    print(f\"{key}: {value:.2f}\" if isinstance(value, float) else f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's analyze correlations between the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate correlation matrix and p-values\n",
    "corr_matrix, p_values = correlation_analysis(X, method='pearson')\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Correlation Matrix:\")\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Visualization\n",
    "\n",
    "Let's create some visualizations to better understand our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the distribution of the target variable\n",
    "fig, ax = plot_histogram(y, bins=20, title='Distribution of Diabetes Progression', \n",
    "                        xlabel='Disease Progression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the correlation matrix\n",
    "fig, ax = plot_correlation_matrix(corr_matrix, p_values=p_values, p_threshold=0.05,\n",
    "                                 title='Feature Correlations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Scatter plot for the most correlated feature with the target\n",
    "# First, find the most correlated feature\n",
    "correlations_with_target = X.corrwith(y)\n",
    "most_correlated = correlations_with_target.abs().sort_values(ascending=False).index[0]\n",
    "\n",
    "# Create the scatter plot\n",
    "fig, ax = plot_scatter(X[most_correlated], y, \n",
    "                      title=f'Relationship between {most_correlated} and Disease Progression',\n",
    "                      xlabel=most_correlated, ylabel='Disease Progression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Now, let's normalize our features and demonstrate handling missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Normalize the features using Z-score normalization\n",
    "X_normalized, scaler = normalization(X, method='zscore')\n",
    "\n",
    "# Display the first few rows of normalized data\n",
    "X_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Artificially introduce some missing values for demonstration\n",
    "X_with_missing = X.copy()\n",
    "np.random.seed(42)\n",
    "for col in X_with_missing.columns[:3]:  # Add missing values to first 3 columns\n",
    "    mask = np.random.random(size=len(X_with_missing)) < 0.1  # 10% missing values\n",
    "    X_with_missing.loc[mask, col] = np.nan\n",
    "\n",
    "# Display the count of missing values\n",
    "print(\"Missing values count:\")\n",
    "print(X_with_missing.isna().sum())\n",
    "\n",
    "# Handle missing values\n",
    "X_cleaned, imputation_info = handle_missing_values(X_with_missing, strategy='mean')\n",
    "\n",
    "# Display imputation information\n",
    "print(\"\\nImputation information:\")\n",
    "for col, info in imputation_info['imputation_values'].items():\n",
    "    print(f\"{col}: {info['strategy']} = {info['value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Machine Learning Utilities\n",
    "\n",
    "Finally, let's demonstrate the machine learning utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split_stratified(\n",
    "    X_normalized, y, test_size=0.2, random_state=42, stratify=False  # Regression task\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Testing set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train a Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model using cross-validation\n",
    "cv_results = cross_validation_metrics(model, X_normalized, y, cv=5, task='regression')\n",
    "\n",
    "# Display the cross-validation results\n",
    "print(\"Cross-validation results:\")\n",
    "for metric, values in cv_results.items():\n",
    "    if metric in ['mean_squared_error', 'mean_absolute_error', 'r2']:\n",
    "        print(f\"{metric}:\")\n",
    "        print(f\"  Test mean: {values['test_mean']:.4f} ± {values['test_std']:.4f}\")\n",
    "        print(f\"  Train mean: {values['train_mean']:.4f} ± {values['train_std']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze feature importance\n",
    "importance_df = feature_importance_analysis(model, X.columns)\n",
    "\n",
    "# Display the feature importances\n",
    "print(\"Feature importances:\")\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances')\n",
    "plt.gca().invert_yaxis()  # Display most important at the top\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the basic functionality of the Python Data Analysis Toolkit. The package provides convenient utilities for:\n",
    "\n",
    "1. Statistical analysis\n",
    "2. Data visualization\n",
    "3. Data preprocessing\n",
    "4. Machine learning tasks\n",
    "\n",
    "These tools can streamline your data science workflow and make common tasks more efficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}